\capitulo{5}{Formas de adaptación del modelo}

Este apartado pretende recoger las diferentes formas de conseguir que el modelo seleccionado emplee el conjunto de datos basado en un conjunto de programas java. Existen diferentes formas de entrenar a nuestro modelo entre las cuales podemos destacar son  RAG (Retrieval-Augmented Generation), LoRA/QLoRA (Low-Rank Adaptation/ Quantum  Low-Rank Adaptation) y el fine-tuning completo (ajuste fino entrenando todos los parámetros del modelo). A continuación, para cada técnica se explica como  funciona, las herramientas típicas para implementarla, requisitos de hardware, ventajas, desventajas y escenarios recomendados de uso. 
\subsection{RAG (Retrieval-Augmented Generation)}
RAG (Generación Aumentada con Recuperación de información) es un método que no altera los parámetros del modelo base, sino que lo potencia proporcionándole información adicional relevante extraída de una base de conocimiento externa en cada consulta. RAG extiende las capacidades del LLM hacia un dominio específico o base de conocimiento interna sin necesidad de reentrenar el modelo. Es una forma rentable de mantener las respuestas del modelo actualizadas, precisas y enfocadas al contexto deseado, porque aprovecha información fuera de los datos originales de entrenamiento del modelo.\cite{RAG-Amazon}

\subsubsection{Funcionamiento}
Con RAG, el proceso de inferencia del modelo se complementa con un buscador que permite recuperar la información actuando antes de la generación de la respuesta. Cuando el usuario realiza una consulta o prompt, este componente de búsqueda utiliza la entrada para recuperar datos relevantes de una fuente externa (por ejemplo, un conjunto de documentos, una base de código o una base de conocimientos)\cite{RAG-Amazon}.

Los datos externos (textos, fragmentos de código, documentación, etc.) suelen haber sido previamente procesados con modelos de embeddings para convertirlos en representaciones numéricas, almacenándose en una base de datos vectorial que permite búsquedas semánticas eficientes\cite{RAG-Amazon}.

Una vez obtenidos los documentos o fragmentos más relevantes mediante la búsqueda (por similitud vectorial, coincidencia semántica, etc.), se aumenta la entrada original del usuario añadiendo ese contenido recuperado al prompt. Finalmente, el modelo de lenguaje genera su respuesta utilizando tanto su conocimiento entrenado como la información específica proporcionada en el prompt \cite{RAG-Amazon}.

En esencia, RAG inyecta conocimiento puntual de la base externa en la entrada del modelo para guiar la generación de una respuesta más informada. Diagrama conceptual de RAG: el modelo de lenguaje (LLM) recibe la consulta del usuario junto con datos relevantes recuperados de una base de conocimiento externa, utilizando ambos para generar una respuesta precisa. El flujo típico es:

\begin{enumerate}
    \item Convertir la pregunta del usuario en una consulta semántica
    \item Buscar en la base de conocimiento vectorial los documentos o fragmentos más relacionados
    \item Añadir esos fragmentos al prompt antes de la generación
\end{enumerate}

De este modo, no se modifica el modelo original, sino que se le proporciona contexto adicional en tiempo de inferencia.

\subsubsection{Requisitos Hardware}
En comparación de los otros metodos de adaptación de modelos que veremos mas adelante se trata realmente de un costo computacional bajo, ya que como hemos visto realmente no realizamos un entrenamiento del modelo sino que realizamos la búsqueda del prompt inicialmente en la base de conocimiento, por lo que el hardware principalmente será necesario para almacenar el modelo original y mantener la estructura de busqueda sobre la base de conocimiento y puede ejecutarse en un entorno doméstico siempre que el modelo por ejemplo no tenga muchos parámetros 

\subsubsection{Ventajas}

\begin{itemize}
    \item No altera el modelo base: No hay que reentrenar el LLM, lo que evita costos de cómputo elevados y riesgos de degradar el modelo. Se aprovecha el conocimiento general del modelo y sólo se le inyecta información específica cuando hace falta.

    \item Información actualizada y específica: Permite al modelo acceder a datos más recientes o especializados que no estaban en su entrenamiento original, así como modificar la base de conocimiento para mantener actualizadas las respuestas que de el LLM.

    \item Control y trazabilidad: podemos controlar la fuente de la información que el modelo usa, imitándola a bases de conocimiento específicas.
\end{itemize}

\subsubsection{Desventajas}

\begin{itemize}
    \item Dependencia del motor de búsqueda: La calidad de las respuestas depende en gran medida de qué tan bien funcione el módulo de búsqueda. Si la base de datos vectorial no devuelve los fragmentos relevantes o está incompleta, el modelo puede seguir alucinando o dando información incorrecta. Requiere por tanto un buen diseño de la base de conocimiento y afinamiento del proceso de búsqueda.

    \item Complejidad del sistema: Comparado con usar sólo un LLM fine-tuneado, RAG introduce componentes adicionales, lo cual puede ser más complejo de implementar y mantener. Hay más partes móviles que pueden fallar o añadir latencia (por ejemplo, la búsqueda vectorial añade pasos extra en cada consulta).

    \item No estamos entrenando el modelo: realmente no estamos adaptando el modelo a nuestras necesidades, simplemente el prompt que nos llega por parte del usuario lo consultamos en la base de conocimiento y este luego se pasa al modelo.
\end{itemize}

\subsubsection{Casos de uso}
\begin{itemize}
    \item Información dinámica o actualizable: Si el conocimiento del dominio cambia con frecuencia (nuevas versiones de software, noticias al día, datos en tiempo real), RAG permite mantener las respuestas actualizadas simplemente actualizando la base de conocimiento, sin tener que re-entrenar el modelo constantemente.

    \item Recursos de cómputo limitados: Cuando no se cuenta con GPUs potentes ni tiempo para entrenamiento, RAG ofrece una forma de adaptar un modelo grande a un contexto específico aprovechando la inferencia solamente. Por ejemplo, empresas que quieran personalizar un chatbot con sus documentos internos, pero no puedan costear entrenar de nuevo un LLM, pueden usar RAG para lograrlo de forma eficiente.

    \item Cuando el modelo necesita contexto para un caso de uso específico: si ya empleamos de base un buen modelo para la tarea concreta, como por ejemplo un modelo el cual esta centrado en código y le damos una base de conocimiento mas concreta entonces obtendremos unas buenas respuestas y encima adaptadas al contexto específico.
\end{itemize}

\subsection{LoRA / QLoRA (Low-Rank Adaptation)}

LoRA (Low-Rank Adaptation) es una técnica de ajuste fino eficiente en parámetros que permite adaptar grandes modelos con un costo computacional muy bajo en comparación con el fine-tuning tradicional. La idea central es congelar los pesos originales del modelo y añadir un pequeño conjunto de parámetros entrenables adicionales que aprenden la tarea o el dominio nuevos. Estos parámetros adicionales toman la forma de adaptadores de rango bajo (low-rank), de ahí el nombre LoRA. Durante el entrenamiento, únicamente se ajustan estos adaptadores (que representan un porcentaje muy pequeño del total de parámetros), manteniendo intactos los pesos originales. Al término, el modelo utiliza los pesos originales más las pequeñas modificaciones aprendidas para generar respuestas especializadas. QLoRA es una variante de LoRA que combina esta idea con cuantización a 4 bits del modelo base para minimizar aún más el uso de memoria. En QLoRA, el modelo preentrenado se carga en formato cuantizado de 4 bits (en lugar de 16 o 32 bits típicos), se mantiene congelado, y se entrenan igualmente adaptadores LoRA encima. Esto permite afinar modelos extraordinariamente grandes usando hardware relativamente accesible, sin pérdida significativa de calidad.\cite{CloudFlare-LoRA}

\subsubsection{Funcionamiento}

En LoRA estándar, las matrices de pesos de determinadas capas del modelo  se complementan con unas matrices de bajo rango que se inicializan de forma que inicialmente no afecten la salida. Durante el entrenamiento, en lugar de actualizar la gran matriz de pesos original W (que permanece congelada), se entrenan dos matrices mucho más pequeñas, $A$ y $B$, cuya multiplicación produce una aproximación de bajo rango de las posibles actualizaciones a $W$. En concreto, $W$ se mantiene fijo y la actualización efectiva es $W + \Delta W$, donde $\Delta W = A \times B$ con $A$ de dimensión (n × r) y $B$ de (r × m) siendo $r$ el rango bajo elegido (mucho menor que n o m). De esta forma, el número de parámetros ajustables es minúsculo comparado con $W$ (aproximadamente $(n+m)r$, en lugar de $nm$). Al finalizar el entrenamiento, el modelo puede usar $W + \Delta W$ como pesos efectivos.\cite{CloudFlare-LoRA}

Para resumir, esto significa que sólo se aprenden las diferencias necesarias para la nueva tarea o dominio, sin alterar los conocimientos originales del modelo. En QLoRA, se agrega un paso previo: cuantizar el modelo base a 4 bits.\cite{HuggingFace-LoRA} El modelo preentrenado en 16 bits (FP16) se convierte a un formato de 4 bits (normalmente NF4, Normalized Float 4, óptimo para pesos distribuidos normalmente). Luego, ese modelo 4-bit se congela y se incorporan los adaptadores LoRA como en el método original.

Durante el entrenamiento, se propagan los gradientes a través del modelo cuantizado congelado hacia los adaptadores, actualizando sólo los pesos de LoRA. La cuantización reduce drásticamente la memoria necesaria para almacenar el modelo, y al no actualizar los pesos cuantizados, se evita degradar su valor original con operaciones de entrenamiento. Para el cálculo de gradientes, QLoRA de hecho decuantiza temporalmente los pesos a 16 bits en el pase hacia atrás, pero sólo calcula gradientes para los parámetros LoRA\cite{HuggingFace-LoRA}

En suma, QLoRA conserva las ventajas de LoRA (pocos parámetros a entrenar) sumándole una compresión del modelo base, logrando una eficiencia excepcional sin prácticamente sacrificar rendimiento (según estudios, QLoRA alcanza resultados a la par del fine-tuning completo en 16 bits en muchos casos).\cite{HuggingFace-LoRA}

\subsubsection{Requisitos Hardware}
LoRA y especialmente QLoRA se diseñaron para reducir radicalmente los requerimientos de memoria y cómputo al adaptar grandes modelos. En la práctica, esto significa que se puede fine-tunear un modelo grande en una sola GPU de gama prosumidor o incluso en GPU de menor VRAM. Por ejemplo, se ha reportado que full fine-tuning de un modelo de ~7B parámetros en FP16 requeriría del orden de 40–60 GB de VRAM, mientras que con LoRA ese mismo modelo puede entrenarse con ~8 GB de VRAM, haciendo posible su uso en entornos domésticos.\cite{Gpu-Finetuning}

\subsubsection{Ventajas}

\begin{itemize}

    \item Extremadamente eficiente en recursos: La principal ventaja es la drástica reducción en uso de VRAM y cómputo durante el entrenamiento. Sólo se ajusta una fracción ínfima de los parámetros del modelo, ademas de que obtiene resultados similares a un finetuning completo.

    \item Conserva el conocimiento original: Dado que el modelo base no se modifica, no hay riesgo de forgetting o de dañar las capacidades generales del LLM. El adaptador aprende sobre el dominio , pero el conocimiento original del modelo (sintaxis general, gramática, conocimientos del mundo) permanece intacto. 

    \item Distribución y despliegue sencillos: Relacionado con lo anterior, compartir o desplegar un modelo adaptado con LoRA es más fácil, ya que sólo se comparten los pesos de los adaptadores (p. ej. vía HuggingFace Hub) que son archivos pequeños, y el destinatario los aplica al modelo base público. Esto facilita la colaboración y la iteración entre equipos, sin intercambiar enormes ficheros de modelo.
\end{itemize}

\subsubsection{Desventajas}

\begin{itemize}
    \item Complejidad de integración en inferencia: Para usar un modelo LoRA, se necesita cargar tanto el modelo base como los pesos del adaptador añadiendo una capa extra de complegidad operacional a tener un modelo completamente ajustado.

    \item No entrena todo el potencial: Si bien LoRA adapta bien el modelo, no permite ajustar todos los matices ya que muchos parámetros quedan sin modificación, por lo que  en conjuntos de datos grandes es mas recomendable realizar un entrenamiento completo de todos los parámetros, ya que LoRA busca cambios en un subespacio limitado.

    \item Limitaciones de QLoRA específicas: En QLoRA, al usar cuantización 4-bit, actualmente no es posible realizar entrenamiento 4-bit en CPU, debe ser en GPU con CUDA.
\end{itemize}

\subsubsection{Casos de uso}

\begin{itemize}
    \item Cuando los recursos son limitados pero se necesita especialización: LoRA/QLoRA es la técnica de elección cuando no se dispone de múltiples GPUs de gran memoria o clusters para entrenamiento.

    \item Proyectos de investigación o prototipos rápidos: Si se desea experimentar con adaptar modelos a distintas tareas de forma ágil.

    \item Múltiples dominios o tareas en un mismo modelo base: Cuando se quiera mantener un solo modelo base y tener variaciones especializadas para distintos casos como un mismo LLM base adaptado por separado a código Java, a generación de documentación técnica.

    \item Cuando se requiere conservar el modelo original intacto: Si por algún motivo necesitamos que el modelo pueda volver a su estado original o mantener la opción de usarlo en tareas generales, LoRA es ideal porque no modifica los pesos base.
\end{itemize}

\subsection{Fine-tuning completo del modelo}

El fine-tuning completo es el enfoque tradicional en el que se entrenan todos los parámetros del modelo de lenguaje en el conjunto de datos específico, ajustando así completamente el modelo a la nueva tarea o dominio. En este caso no se introduce arquitectura adicional: simplemente se toma el modelo preentrenado existente y se continúa entrenando con los datos proporcionados (normalmente con una tasa de aprendizaje baja para no destruir lo ya aprendido). Al finalizar, todos los pesos del modelo pueden haberse actualizado para reflejar los patrones del nuevo conjunto de datos. Este método convierte esencialmente al modelo generalista en un modelo especializado aprendiendo directamente de los datos específicos. Las principales desventajas son que es costoso a nivel de recursos y tiene sus riesgos de cara al modelo final.

\subsubsection{Funcionamiento}

Es similar a cuando se entrena el modelo original pero partiendo de los pesos ya aprendidos para los parámetros en pre-entrenamiento. Se formula un objetivo  y se usa el algoritmo de optimización  para ajustar los gradientes de todos los parámetros en cada paso, minimizando la función de pérdida en el conjunto de datos específico. Durante este proceso, el modelo puede adaptar todos sus niveles de representación: desde capas inferiores (aprendiendo algunas sintaxis específicas del código Java del conjunto de datos que estamos usando) hasta las superiores (quizás adoptando cierto estilo en las respuestas

Tras el fine-tuning, se obtiene un nuevo modelo (un nuevo conjunto completo de pesos) especializado en los datos proporcionados. En nuestro caso específico si afinamos completamente un modelo con un con un conjunto de datos de código Java, el resultado es un modelo cuyo conocimiento del lenguaje Java y estará embebido en todos sus parámetros.

\subsubsection{Requisitos Hardware}
El fine-tuning completo de LLMs es notoriamente intensivo en recursos. Se necesita suficiente VRAM (memoria de GPU) para alojar el modelo completo y sus gradientes y los estados del optimizador, además de las activaciones durante el pase hacia adelante. Esto se acumula rápidamente: por ejemplo, un modelo de 7 mil millones de parámetros ocupa unos ~14 GB en FP16 solo para los pesos; el optimizador requiere de mas recursos, y las activaciones/gradientes de una pasada pueden requerir otros ~10 GB, llevando el total a on the order of 40–60 GB de VRAM para entrenar un modelo de 7B\cite{Gpu-Finetuning}

\subsubsection{Ventajas}

\begin{itemize}
    \item Modelo autónomo especializado: El resultado del fine-tuning completo es un modelo especializado independiente, que no necesita componentes adicionales ni contexto externo para desempeñarse en el dominio. Toda la información relevante del dataset queda embebida en sus pesos.

    \item Adaptación de todos los pesos de los parámetros: Al actualizar todos los parámetros, el modelo tiene máxima capacidad de absorber el nuevo conocimiento o adaptar sus habilidades. No está limitado por un subespacio de ajustes (como en LoRA). Esto puede traducirse en un rendimiento ligeramente superior en la tarea específica cuando se dispone de suficientes datos.

    \item Personalización profunda del comportamiento: Más allá de conocimiento actual, al fine-tunear completo es posible cambiar comportamientos intrínsecos del modelo.
\end{itemize}

\subsubsection{Desventajas}
\begin{itemize}

    \item Coste computacional alto: es de los tres métodos el que mayor conste computacional tiene tanto en hardware como en tiempo

    \item Riesgo de sobreajuste y forgetting: Al actualizarlo todo, existe el riesgo de sobreajustar al conjunto de datos específico, especialmente si este no es muy grande. El modelo puede terminar "memorizando" en exceso patrones o hasta ejemplos enteros del dataset o incluso el  modelo puede perder parte de su conocimiento general al sobrescribirlo con la nueva información.

    \item Necesidad de gran cantidad de datos de calidad: Para aprovechar un fine-tuning completo sin arruinar el modelo, se suele requerir un conjunto de datos amplio y bien adaptado al dominio.
\end{itemize}
